# PolyMorph

**Multi-tool morphological analysis with suffix-first parsing**

[![Tests](https://img.shields.io/badge/tests-104%2F105%20passing-brightgreen)]()
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)]()
[![License](https://img.shields.io/badge/license-MIT-blue)]()

PolyMorph is a comprehensive command-line tool for morphological analysis that intelligently combines multiple NLP approaches:

- **Suffix-first parsing** using a curated suffix database
- **spaCy** for neural POS tagging and morphological features
- **WordNet** for lemmatization and sense definitions

## Features

ðŸŽ¯ **Multi-Tool Integration** - Combines suffix database, spaCy, and WordNet for comprehensive analysis

ðŸ¤ **Consensus Detection** - Intelligently aggregates results and detects agreement/disagreement across tools

ðŸ“Š **Multiple Output Formats** - JSON, plain text, and CSV export

âš¡ **Batch Processing** - Analyze multiple words efficiently

ðŸŽ¨ **Flexible Formatting** - Pretty-print, compact mode, field filtering

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/PolyMorph.git
cd PolyMorph

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Download WordNet data
python scripts/setup_wordnet.py
```

### Basic Usage

```bash
# Identify suffix
python3 cli.py suffix --word happiness --pretty

# Full morphological analysis
python3 cli.py analyze --word running --format text

# Batch processing
python3 cli.py analyze --words happy,sadness,running

# CSV export
python3 cli.py analyze --words happy,sad --format csv
```

## Examples

### Suffix Identification

```bash
$ python3 cli.py suffix --word happiness --pretty
{
  "word": "happiness",
  "suffix": "-ness",
  "stem": "happi",
  "POS": "noun",
  "base_POS": ["adjective"],
  "confidence": 0.7
}
```

### Full Analysis with Text Output

```bash
$ python3 cli.py analyze --word running --format text
Word: running
Lemma: run
POS: [suffix:verb, spacy:verb, wordnet:noun] (no consensus)
Stem: runn
Suffix: -ing
Confidence: 0.90
Tag: VBG
Morphological Features: Aspect=Prog, Tense=Pres, VerbForm=Part
```

### Batch Processing

```bash
$ python3 cli.py analyze --words happy,sadness,beautiful --format text
Word: happy
Lemma: happy
POS: adjective
Stem: happy
Confidence: 0.00

----------------------------------------

Word: sadness
Lemma: sadness
POS: noun
Stem: sad
Suffix: -ness
Base POS: adjective
Confidence: 0.90

----------------------------------------

Word: beautiful
Lemma: beautiful
POS: adjective
Stem: beauti
Suffix: -ful
Confidence: 0.70
```

### Field Filtering

```bash
$ python3 cli.py analyze --word beautiful --fields word,lemma,POS,suffix
{
  "word": "beautiful",
  "lemma": "beautiful",
  "POS": "adjective",
  "suffix": "-ful"
}
```

## Commands

### `suffix` - Suffix Identification

Identify and analyze suffixes in words.

```bash
python3 cli.py suffix --word WORD [OPTIONS]

Options:
  --word WORD              Word to analyze (required)
  --db PATH               Path to suffix database
  --min-stem INT          Minimum stem length (default: 2)
  --format {json,text,csv} Output format (default: json)
  --pretty                Pretty-print JSON
  --compact               Compact JSON (no whitespace)
  --fields FIELDS         Comma-separated fields to include
  --no-color              Disable color output
```

### `analyze` - Full Morphological Analysis

Complete analysis using suffix database, spaCy, and WordNet.

```bash
python3 cli.py analyze --word WORD [OPTIONS]
python3 cli.py analyze --words WORD1,WORD2,... [OPTIONS]

Options:
  --word WORD             Single word to analyze
  --words WORDS           Comma-separated words for batch processing
  --context SENTENCE      Context sentence for disambiguation
  --format {json,text,csv} Output format (default: json)
  --pretty                Pretty-print JSON
  --compact               Compact JSON
  --fields FIELDS         Comma-separated fields to include
  --no-color              Disable color output
```

## Output Formats

### JSON (Default)
Structured data ideal for programmatic processing:
```json
{
  "word": "happiness",
  "lemma": "happiness",
  "POS": "noun",
  "stem": "happi",
  "suffix": "-ness",
  "confidence": 0.9
}
```

### Text
Human-readable format:
```
Word: happiness
Lemma: happiness
POS: noun
Stem: happi
Suffix: -ness
Confidence: 0.90
```

### CSV
Spreadsheet-compatible export:
```csv
word,lemma,POS,stem,suffix,confidence
happiness,happiness,noun,happi,-ness,0.9
```

## Architecture

### Multi-Tool Integration

PolyMorph combines three complementary approaches:

1. **Suffix Database** (`scripts/identify_suffix.py`)
   - Longest-match heuristic
   - 200+ English suffixes
   - Confidence scoring based on frequency

2. **spaCy** (`scripts/extract_spacy_features.py`)
   - Neural POS tagging
   - Morphological features
   - Dependency parsing

3. **WordNet** (`scripts/extract_wordnet_features.py`)
   - Lemmatization
   - Word sense definitions
   - Synonym relationships

### Result Aggregation

When tools agree:
```json
{
  "POS": "noun"
}
```

When tools disagree:
```json
{
  "POS": {
    "suffix": "verb",
    "spacy": "verb",
    "wordnet": "noun",
    "consensus": false
  }
}
```

## Project Structure

```
PolyMorph/
â”œâ”€â”€ cli.py                    # Main CLI entry point
â”œâ”€â”€ formatters/               # Output formatters
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_formatter.py
â”‚   â”œâ”€â”€ text_formatter.py
â”‚   â””â”€â”€ csv_formatter.py
â”œâ”€â”€ scripts/                  # Analysis scripts
â”‚   â”œâ”€â”€ identify_suffix.py
â”‚   â”œâ”€â”€ extract_spacy_features.py
â”‚   â”œâ”€â”€ extract_wordnet_features.py
â”‚   â””â”€â”€ setup_wordnet.py
â”œâ”€â”€ data/                     # Suffix database
â”‚   â””â”€â”€ unified_suffixes.json
â”œâ”€â”€ tests/                    # Test suite (104/105 passing)
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ helpers/
â””â”€â”€ docs/                     # Documentation
```

## Development

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test category
pytest tests/unit/ -v
pytest tests/integration/ -v

# Run with coverage
pytest tests/ --cov=cli --cov=formatters --cov-report=html
```

### Test Coverage

- **104/105 tests passing (99.05%)**
- Unit tests: 48/48 âœ…
- Integration tests: 56/57 âœ…

## Requirements

- Python 3.8+
- spaCy >= 3.0
- NLTK >= 3.6
- pytest (for testing)

See `requirements.txt` for complete list.

## Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Citation

If you use PolyMorph in academic work, please cite:

```bibtex
@software{polymorph2026,
  title = {PolyMorph: Multi-tool Morphological Analysis},
  author = {Preston},
  year = {2026},
  url = {https://github.com/yourusername/PolyMorph}
}
```

## Acknowledgments

- Suffix database curated from linguistic resources
- Built with spaCy and NLTK
- Developed using Test-Driven Development (TDD)

## See Also

- [IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md) - Detailed implementation report
- [IMPLEMENTATION_GUIDE.md](IMPLEMENTATION_GUIDE.md) - Step-by-step implementation guide
- [TEST_FAILURE_REPORT.md](TEST_FAILURE_REPORT.md) - Original TDD test specifications

---

**PolyMorph** - Because words have many forms ðŸ¦Ž

## Overview

This plugin provides a unified interface to multiple morphological analysis tools, combining their strengths to extract maximum information about word structure, meaning, and grammatical properties.

### Key Features

- **Suffix-First Parsing** - Analyze words from suffix to stem for efficient POS disambiguation
- **Multi-Tool Integration** - Combines spaCy, WordNet, suffix lexicons, FST, and more
- **Property Extraction** - Request specific morphological properties with automatic routing
- **Reversed Trie Index** - Memory-efficient suffix-to-stem lookups using marisa-trie
- **Data Acquisition** - Smart downloading with bandwidth-friendly search-before-download
- **Disambiguation** - Aggregate results from multiple tools with confidence scoring

## Architecture

Based on the research document "Morphological Analysis Infrastructure: Suffix-First Parsing", this plugin implements:

1. **Suffix Lexicons** - MorphoLex, NIH Lexical Tools, WordNet morphy rules
2. **Trie-Based Indexing** - Reversed tries for O(m) suffix lookups
3. **FST Morphology** - Bidirectional finite-state transducers (Pynini, OpenFST)
4. **Statistical Analysis** - spaCy morphologizer and POS tagger
5. **Rule-Based Analysis** - FreeLing suffix segmentation
6. **Unsupervised Segmentation** - Morfessor for discovering morphemes

### Suffix-First Approach

Traditional morphological analyzers search the entire lexicon. This plugin uses a **suffix-first constraint**:

1. Identify the suffix (e.g., "-ness" in "happiness")
2. Determine suffix POS (e.g., "-ness" â†’ noun)
3. Search only noun stems (drastically reduced search space)
4. Verify stem exists (e.g., "happi" or "happy")
5. Return analysis with high confidence

**Benefits:**
- Reduces lexicon search space by 80-95%
- Strong POS disambiguation (suffix determines POS in ~90% of cases)
- Fast: O(m) suffix match + O(log n) trie lookup
- Handles out-of-vocabulary words better than pure dictionary lookup

## Installation

### Quick Start

```bash
# Plugin is already installed at:
# ~/.claude/plugins/morphological-analysis-infrastructure

# Run setup script
cd ~/.claude/plugins/morphological-analysis-infrastructure
./setup.sh
```

### Manual Setup

```bash
# 1. Verify Python packages (already installed in your conda env)
python -c "import spacy, nltk, marisa_trie; print('âœ“ Core packages installed')"

# 2. Download spaCy model (if needed)
python -m spacy download en_core_web_sm

# 3. Acquire data (interactive - searches before downloading)
python scripts/acquire_data_interactive.py

# 4. Build suffix database
python scripts/build_suffix_database.py

# 5. Build reversed trie
python scripts/build_reversed_trie.py \
    --wordlist /usr/share/dict/words \
    --output data/reversed_stems.trie
```

## Usage

### Via Skills (Recommended)

```bash
# The plugin provides these user-invocable skills:

# Main router (auto-detects query type)
/morph-router "What is the suffix and POS of 'happiness'?"

# Specific tools
/suffix-lexicon --word "happiness"
/trie-suffix-lookup --suffix "-ness"
/spacy-morph --text "The cats are running"
/property-extractor --word "running" --properties lemma,POS,suffix
```

### Via Scripts (Direct Access)

```bash
# Identify suffix
python scripts/identify_suffix.py --word "happiness"

# Extract all properties
python scripts/extract_properties.py --word "unbelievable"

# Query WordNet
python scripts/extract_wordnet_features.py --word "running"

# Query spaCy
python scripts/extract_spacy_features.py --word "running" --context "They are running fast"

# Suffix-to-stems lookup
python scripts/query_suffix_trie.py --suffix "-ness" --trie data/reversed_stems.trie
```

## Skills Reference

### 1. morph-router
Main entry point. Auto-detects query type and routes to appropriate tools.

**Use Cases:**
- "Analyze 'happiness' morphologically"
- "What words end in '-able'?"
- "Get all properties for 'running'"

### 2. suffix-lexicon
Manages suffix databases (MorphoLex, NIH, WordNet rules).

**Use Cases:**
- Query suffix properties
- Identify suffix in word
- Build/update suffix database

### 3. trie-suffix-lookup
Reversed trie operations for suffix-first lookups.

**Use Cases:**
- Find all stems with given suffix
- Verify stem exists
- Fast suffix-to-stem matching

### 4. property-extractor
Unified interface to extract specific morphological properties.

**Use Cases:**
- "Get lemma, POS, and suffix for 'believable'"
- Batch property extraction
- Custom property definitions

### 5. spacy-morph
Statistical morphological analysis via spaCy.

**Use Cases:**
- POS tagging
- Morphological features (Number, Tense, etc.)
- Dependency parsing integration

### 6. wordnet-morphy
WordNet-based lemmatization and POS disambiguation.

**Use Cases:**
- Lemmatize with POS hint
- Get word senses
- Inflectional suffix handling

### 7. data-acquisition
Smart data download with search-before-download.

**Use Cases:**
- First-time setup
- Update databases
- Locate existing data files

## Data Sources

### Included/Automatic
- **WordNet** - Via NLTK (auto-download)
- **English Word Lists** - `/usr/share/dict/words`
- **spaCy Models** - `en_core_web_sm` (auto-download)

### Downloaded
- **MorphoLex** - GitHub (free, ~5 MB)
  - 68k words with morphological segmentation
  - Suffix productivity statistics
  - Source: `hugomailhot/MorphoLex-en`

- **NIH Lexical Tools** - Web scraping (free, <1 MB)
  - ~100-200 derivational suffixes
  - Suffixâ†’POS mappings
  - Transformation rules

### Licensed (Optional)
- **CELEX** - Requires LDC license
  - Comprehensive morphology for 50k words
  - Not required for basic functionality

## File Structure

```
~/.claude/plugins/morphological-analysis-infrastructure/
â”œâ”€â”€ plugin.json                 # Plugin metadata
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ setup.sh                    # Quick setup script
â”‚
â”œâ”€â”€ skills/                     # Skill definitions
â”‚   â”œâ”€â”€ morph-router.md         # Main router
â”‚   â”œâ”€â”€ suffix-lexicon.md       # Suffix database management
â”‚   â”œâ”€â”€ trie-suffix-lookup.md   # Reversed trie operations
â”‚   â”œâ”€â”€ property-extractor.md   # Property aggregation
â”‚   â”œâ”€â”€ spacy-morph.md          # spaCy integration
â”‚   â”œâ”€â”€ wordnet-morphy.md       # WordNet integration
â”‚   â”œâ”€â”€ data-acquisition.md     # Data download
â”‚   â”œâ”€â”€ fst-morphology.md       # FST tools
â”‚   â”œâ”€â”€ freeling-analyzer.md    # FreeLing integration
â”‚   â””â”€â”€ morfessor-segment.md    # Morfessor segmentation
â”‚
â”œâ”€â”€ scripts/                    # Python scripts
â”‚   â”œâ”€â”€ identify_suffix.py      # Suffix identification
â”‚   â”œâ”€â”€ extract_spacy_features.py
â”‚   â”œâ”€â”€ extract_wordnet_features.py
â”‚   â”œâ”€â”€ extract_properties.py   # Property extractor
â”‚   â”œâ”€â”€ build_reversed_trie.py  # Trie construction
â”‚   â”œâ”€â”€ query_suffix_trie.py    # Trie queries
â”‚   â”œâ”€â”€ download_morpholex.py   # MorphoLex downloader
â”‚   â”œâ”€â”€ download_nih_suffixes.py
â”‚   â”œâ”€â”€ acquire_data_interactive.py  # Interactive setup
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ data/                       # Data files (created during setup)
    â”œâ”€â”€ MorphoLex_en.xlsx       # MorphoLex database
    â”œâ”€â”€ nih_suffixes.json       # NIH suffix rules
    â”œâ”€â”€ morpholex_suffixes.json # Processed suffixes
    â”œâ”€â”€ unified_suffixes.json   # Merged suffix database
    â”œâ”€â”€ reversed_stems.trie     # Reversed trie index
    â””â”€â”€ data_sources.json       # Configuration
```

## Examples

### Example 1: Analyze "happiness"

```bash
python scripts/extract_properties.py --word "happiness"
```

**Output:**
```json
{
  "word": "happiness",
  "properties": {
    "lemma": {
      "value": "happiness",
      "confidence": 0.95,
      "sources": ["spacy", "wordnet"],
      "consensus": true
    },
    "POS": {
      "value": "noun",
      "confidence": 0.95,
      "sources": ["spacy", "wordnet", "suffix_lexicon"],
      "consensus": true
    },
    "suffix": {
      "value": "-ness",
      "confidence": 0.9,
      "sources": ["suffix_lexicon"],
      "consensus": true
    },
    "stem": {
      "value": "happi",
      "confidence": 0.9,
      "sources": ["suffix_lexicon"],
      "consensus": true
    },
    "base_POS": {
      "value": ["adjective"],
      "confidence": 0.9,
      "sources": ["suffix_lexicon"],
      "consensus": true
    }
  }
}
```

### Example 2: Find words ending in "-able"

```bash
python scripts/query_suffix_trie.py \
    --suffix "-able" \
    --trie data/reversed_stems.trie \
    --max 20
```

**Output:**
```json
[
  {"word": "believable", "stem": "believ", "suffix": "-able"},
  {"word": "readable", "stem": "read", "suffix": "-able"},
  {"word": "doable", "stem": "do", "suffix": "-able"},
  ...
]
```

### Example 3: Batch Analysis

```bash
python scripts/batch_extract_properties.py \
    --words "happiness,running,unbelievable" \
    --properties lemma,POS,suffix
```

## Advanced Topics

### Custom Suffix Database

Add your own suffixes:

```json
{
  "suffixes": {
    "-custom": {
      "POS": "noun",
      "source_POS": ["verb"],
      "category": "derivational",
      "meaning": "your definition",
      "examples": ["example1", "example2"]
    }
  }
}
```

### FST Integration

Build custom FST morphological analyzer:

```python
import pynini

# Create suffix FST
suffix_fst = pynini.string_map({
    "-ness": "NOUN_SUFFIX",
    "-able": "ADJ_SUFFIX",
    # ...
})

# Compose with stem lexicon
analyzer = stem_fst + suffix_fst
```

### spaCy Custom Component

Add suffix-first analysis to spaCy pipeline:

```python
import spacy

nlp = spacy.load('en_core_web_sm')
nlp.add_pipe('suffix_first_tagger', before='tagger')

doc = nlp("The happiness is overwhelming")
for token in doc:
    print(f"{token.text}: {token._.suffix_pos}")
```

## Performance

- **Trie Build:** ~0.1s for 50k words
- **Suffix Lookup:** ~1Âµs per query
- **Property Extraction:** ~10-50ms per word (depends on tools used)
- **Memory:** <1 MB for trie, ~5 MB for suffix database

## Troubleshooting

### "Suffix database not found"
Run data acquisition:
```bash
python scripts/acquire_data_interactive.py
```

### "spaCy model not found"
Download model:
```bash
python -m spacy download en_core_web_sm
```

### "WordNet data not found"
```python
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
```

## Contributing

To add new analyzers or tools:

1. Create skill definition in `skills/`
2. Create corresponding script in `scripts/`
3. Update `morph-router` to include new tool
4. Add to `plugin.json` dependencies

## References

Based on the research document:
**"Morphological Analysis Infrastructure: Suffix-First Parsing"**

Key papers and resources cited:
- MorphoLex database (SÃ¡nchez-GutiÃ©rrez et al., 2017)
- NIH Lexical Tools suffix list
- WordNet morphy algorithm
- FreeLing morphological analyzer
- Buckwalter Arabic Morphological Analyzer (suffix-first approach)

## License

Plugin code: MIT License

Data sources have their own licenses:
- MorphoLex: CC BY 4.0
- WordNet: WordNet License
- spaCy models: MIT License

---

**Author:** Preston
**Version:** 1.0.0
**Last Updated:** 2026-02-22
